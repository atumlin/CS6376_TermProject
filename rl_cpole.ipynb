{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym in /usr/local/lib/python3.11/site-packages (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.11/site-packages (from gym) (1.26.2)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/site-packages (from gym) (3.0.0)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.11/site-packages (from gym) (0.0.8)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/site-packages (2.1.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/site-packages (from torch) (4.8.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/site-packages (3.8.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/site-packages (from matplotlib) (4.45.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: numpy<2,>=1.21 in /usr/local/lib/python3.11/site-packages (from matplotlib) (1.26.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/annetumlin/Library/Python/3.11/lib/python/site-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/site-packages (from matplotlib) (10.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/site-packages (from matplotlib) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/annetumlin/Library/Python/3.11/lib/python/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/annetumlin/Library/Python/3.11/lib/python/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/site-packages (0.16.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/site-packages (from torchvision) (1.26.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/site-packages (from torchvision) (2.31.0)\n",
      "Requirement already satisfied: torch==2.1.1 in /usr/local/lib/python3.11/site-packages (from torchvision) (2.1.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/site-packages (from torchvision) (10.1.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/site-packages (from torch==2.1.1->torchvision) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/site-packages (from torch==2.1.1->torchvision) (4.8.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/site-packages (from torch==2.1.1->torchvision) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/site-packages (from torch==2.1.1->torchvision) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/site-packages (from torch==2.1.1->torchvision) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/site-packages (from torch==2.1.1->torchvision) (2023.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/site-packages (from requests->torchvision) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/site-packages (from requests->torchvision) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/site-packages (from requests->torchvision) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/site-packages (from requests->torchvision) (2023.11.17)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/site-packages (from jinja2->torch==2.1.1->torchvision) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.11/site-packages (from sympy->torch==2.1.1->torchvision) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install gym\n",
    "%pip install torch \n",
    "%pip install matplotlib\n",
    "%pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install packages\n",
    "import gym\n",
    "import copy\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from IPython.display import clear_output\n",
    "import math\n",
    "import torchvision.transforms as T\n",
    "import numpy as np\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.envs.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting Function - REPLACE LATER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_res(values, title=''):\n",
    "    ''' Plot the reward curve and histogram of results over time.'''\n",
    "    # Update the window after each episode\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    # Define the figure\n",
    "    f, ax = plt.subplots(nrows=1, ncols=2, figsize=(12,5))\n",
    "    f.suptitle(title)\n",
    "    ax[0].plot(values, label='score per run')\n",
    "    ax[0].axhline(195, c='red',ls='--', label='goal')\n",
    "    ax[0].set_xlabel('Episodes')\n",
    "    ax[0].set_ylabel('Reward')\n",
    "    x = range(len(values))\n",
    "    ax[0].legend()\n",
    "    # Calculate the trend\n",
    "    try:\n",
    "        z = np.polyfit(x, values, 1)\n",
    "        p = np.poly1d(z)\n",
    "        ax[0].plot(x,p(x),\"--\", label='trend')\n",
    "    except:\n",
    "        print('')\n",
    "\n",
    "    # Plot the histogram of results\n",
    "    ax[1].hist(values[-50:])\n",
    "    ax[1].axvline(195, c='red', label='goal')\n",
    "    ax[1].set_xlabel('Scores per Last 50 Episodes')\n",
    "    ax[1].set_ylabel('Frequency')\n",
    "    ax[1].legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN():\n",
    "    ''' Deep Q Neural Network class. '''\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=64, lr=0.05):\n",
    "            self.criterion = torch.nn.MSELoss()\n",
    "            self.model = torch.nn.Sequential(\n",
    "                            torch.nn.Linear(state_dim, hidden_dim),\n",
    "                            torch.nn.LeakyReLU(),\n",
    "                            torch.nn.Linear(hidden_dim, hidden_dim*2),\n",
    "                            torch.nn.LeakyReLU(),\n",
    "                            torch.nn.Linear(hidden_dim*2, action_dim)\n",
    "                    )\n",
    "            self.optimizer = torch.optim.Adam(self.model.parameters(), lr)\n",
    "\n",
    "\n",
    "\n",
    "    def update(self, state, y):\n",
    "        \"\"\"Update the weights of the network given a training sample. \"\"\"\n",
    "        y_pred = self.model(torch.Tensor(state))\n",
    "        loss = self.criterion(y_pred, Variable(torch.Tensor(y)))\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "\n",
    "    def predict(self, state):\n",
    "        \"\"\" Compute Q values for all actions using the DQL. \"\"\"\n",
    "        with torch.no_grad():\n",
    "            return self.model(torch.Tensor(state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(env, model, episodes, gamma=0.9,\n",
    "               epsilon=0.3, eps_decay=0.99,\n",
    "               replay=False, replay_size=20,\n",
    "               title = 'DQL', double=False,\n",
    "               n_update=10, soft=False, verbose=True):\n",
    "    \"\"\"Deep Q Learning algorithm using the DQN. \"\"\"\n",
    "    final = []\n",
    "    memory = []\n",
    "    episode_i=0\n",
    "    sum_total_replay_time=0\n",
    "    for episode in range(episodes):\n",
    "        episode_i+=1\n",
    "        if double and not soft:\n",
    "            # Update target network every n_update steps\n",
    "            if episode % n_update == 0:\n",
    "                model.target_update()\n",
    "        if double and soft:\n",
    "            model.target_update()\n",
    "\n",
    "        # Reset state\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        total = 0\n",
    "\n",
    "        while not done:\n",
    "            # Implement greedy search policy to explore the state space\n",
    "            if random.random() < epsilon:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                q_values = model.predict(state)\n",
    "                action = torch.argmax(q_values).item()\n",
    "\n",
    "            # Take action and add reward to total\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "            # Update total and memory\n",
    "            total += reward\n",
    "            memory.append((state, action, next_state, reward, done))\n",
    "            q_values = model.predict(state).tolist()\n",
    "\n",
    "            if done:\n",
    "                if not replay:\n",
    "                    q_values[action] = reward\n",
    "                    # Update network weights\n",
    "                    model.update(state, q_values)\n",
    "                break\n",
    "\n",
    "            if replay:\n",
    "                t0=time.time()\n",
    "                # Update network weights using replay memory\n",
    "                model.replay(memory, replay_size, gamma)\n",
    "                t1=time.time()\n",
    "                sum_total_replay_time+=(t1-t0)\n",
    "            else:\n",
    "                # Update network weights using the last step only\n",
    "                q_values_next = model.predict(next_state)\n",
    "                q_values[action] = reward + gamma * torch.max(q_values_next).item()\n",
    "                model.update(state, q_values)\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "        # Update epsilon\n",
    "        epsilon = max(epsilon * eps_decay, 0.01)\n",
    "        final.append(total)\n",
    "        plot_res(final, title)\n",
    "\n",
    "        if verbose:\n",
    "            print(\"episode: {}, total reward: {}\".format(episode_i, total))\n",
    "            if replay:\n",
    "                print(\"Average replay time:\", sum_total_replay_time/episode_i)\n",
    "\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of states\n",
    "n_state = env.observation_space.shape[0]\n",
    "# Number of actions\n",
    "n_action = env.action_space.n\n",
    "# Number of episodes\n",
    "episodes = 150\n",
    "# Number of hidden nodes in the DQN\n",
    "n_hidden = 50\n",
    "# Learning rate\n",
    "lr = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "expected sequence of length 4 at dim 1 (got 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/annetumlin/Downloads/research/CS6376_TermProject/rl_cpole.ipynb Cell 10\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/annetumlin/Downloads/research/CS6376_TermProject/rl_cpole.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Get DQN results\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/annetumlin/Downloads/research/CS6376_TermProject/rl_cpole.ipynb#X12sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m simple_dqn \u001b[39m=\u001b[39m DQN(n_state, n_action, n_hidden, lr)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/annetumlin/Downloads/research/CS6376_TermProject/rl_cpole.ipynb#X12sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m simple \u001b[39m=\u001b[39m q_learning(env, simple_dqn, episodes, gamma\u001b[39m=\u001b[39;49m\u001b[39m.9\u001b[39;49m, epsilon\u001b[39m=\u001b[39;49m\u001b[39m0.3\u001b[39;49m)\n",
      "\u001b[1;32m/Users/annetumlin/Downloads/research/CS6376_TermProject/rl_cpole.ipynb Cell 10\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/annetumlin/Downloads/research/CS6376_TermProject/rl_cpole.ipynb#X12sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     action \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39msample()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/annetumlin/Downloads/research/CS6376_TermProject/rl_cpole.ipynb#X12sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/annetumlin/Downloads/research/CS6376_TermProject/rl_cpole.ipynb#X12sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     q_values \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mpredict(state)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/annetumlin/Downloads/research/CS6376_TermProject/rl_cpole.ipynb#X12sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     action \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39margmax(q_values)\u001b[39m.\u001b[39mitem()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/annetumlin/Downloads/research/CS6376_TermProject/rl_cpole.ipynb#X12sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39m# Take action and add reward to total\u001b[39;00m\n",
      "\u001b[1;32m/Users/annetumlin/Downloads/research/CS6376_TermProject/rl_cpole.ipynb Cell 10\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/annetumlin/Downloads/research/CS6376_TermProject/rl_cpole.ipynb#X12sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\" Compute Q values for all actions using the DQL. \"\"\"\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/annetumlin/Downloads/research/CS6376_TermProject/rl_cpole.ipynb#X12sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/annetumlin/Downloads/research/CS6376_TermProject/rl_cpole.ipynb#X12sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel(torch\u001b[39m.\u001b[39;49mTensor(state))\n",
      "\u001b[0;31mValueError\u001b[0m: expected sequence of length 4 at dim 1 (got 0)"
     ]
    }
   ],
   "source": [
    "# Get DQN results\n",
    "simple_dqn = DQN(n_state, n_action, n_hidden, lr)\n",
    "simple = q_learning(env, simple_dqn, episodes, gamma=.9, epsilon=0.3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
